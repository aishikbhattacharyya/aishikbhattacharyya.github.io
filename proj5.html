<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 180 Project 5</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>

<h1 align="middle">CS 180: Intro to Computer Vision and Computational Photography, Fall 2024</h1>
<h1 align="middle">Project 5: Fun With Diffusion Models!</h1>
<h2 align="middle">Aishik Bhattacharyya</h2>

<div>

<h2 align="middle">Overview</h2>
	
In this project, our goal is to implement and deploy diffusion models. Part A focuses on adding noise and denoising images through various techniques. It also looks at how certain prompt embeddings lead to unique images, as we play around with inpainting and making hybrid images. Part B is more advanced. The goal here is to train our own models on the MNIST dataset. There are different UNet architectures being used, leading to different training loss graphs and results.

<h1 align="middle">Part A: The Power of Diffusion Models!</h1>

<h3 align="middle">Setup</h3>
Here are some pictures from the rooftop of an apartment.

A homography is a 3x3 matrix which represents a transformation between a pair of images. We can recover a homography by using pairs of corresponding points. The points on the pictures above represent the corresponding points. To implement this function, I went through each pair of points and created a system of equations to obtain the homography matrix’s coefficients, of which there will be 9 values. I then solved it with a least squares solver and reshaped it into a 3x3 matrix.
<br>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part0_1.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	  </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 1.1: Implementing the Forward Process</h3>
In this part, we aim to take a clean image and produce a noisy image by sampling from a Gaussian. To accomplish this, I first get the alpha bar by getting the correct alpha_cumprod value at its respective timestep. Then, I calculate a random epsilon value and use all this info in equation 2 to get the noisy image.
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/eq2.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Equation 2</figcaption>
      </td>
	<td>
        <img src="proj5_images/part1_1.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Noisy Campanile at various t-levels</figcaption>
      </td>
  </table>
	  
<h3 align="middle">Part 1.2: Classical Denoising</h3>
Now, we use Gaussian blur filtering to try to remove the noise with a kernel size of 5 and sigma of 1. This is primarily accomplished using torchvision.transforms.functional.gaussian_blur.
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1_2.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Gaussian Blur Denoising at various t-levels</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.3: One Step Denoising</h3>
We use a pre-trained diffusion model to denoise, which is a UNet that’s been trained on a very large dataset. Using the UNet, we first estimate the noise in the noisy image. Then, we use the formula in equation 2 (as shown in 1.1) to create the clean image given the noisy image, using the noise estimate in the place of epsilon. This gives us a blurry image without the noise seen in 1.1, as well as providing a better output than 1.2.
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1_3.PNG" align="middle" width="400px"/>
        <figcaption align="middle">One-Step Denoised Campanile at various t-levels</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.4: Iterative Denoising</h3>
In this part, instead of one-step denoising, we plan to denoise iteratively. We create a list of timesteps from 990 to 0, with intervals of 30. So at each denoising step, we use formula 3 to reduce the noise within the current iteration of the image. This is accomplished by calculating the alpha and beta values at the current and incoming timestep and running unet to get the noise estimate of the current image.
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/eq3.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Equation 3</figcaption>
      </td>
  </table>
</div>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.4_fifths.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Noisy image every 5th loop of denoising</figcaption>
      </td>
	<td>
        <img src="proj5_images/part1.4_clean.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Final Image for Iterative Denoising</figcaption>
</td>
  </table>
</div>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.4_onestep.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Final Image for Single Denoising Step</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1.4_gaussian.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Final Image with Gaussian Blurring</figcaption>
      </td>
  </table>
</div>


<h3 align="middle">Part 1.

<h3 align="middle">Part 1.5: Diffusion Model Sampling</h3>
I use torch.randn to make noise and then call the iterative_denoise from the last part to generate the image.
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.5.PNG" align="middle" width="400px"/>
        <figcaption align="middle">5 Sampled Images</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.6: Classifier Free Guidance</h3>
To use this technique, we use a lot of code written for iterative denoising. The difference is we have conditional and unconditional prompt embeddings. The unconditional prompt embedding is just null and we calculate the Unet for both these values instead of just one before. Then, we pull the noise estimates for both the Unets and use the formula below with the pre-specified scaling factor. The rest is the same.<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.6.PNG" align="middle" width="400px"/>
        <figcaption align="middle">5 Sampled Images</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.7: Image-to-image Translation</h3>
In this section, we add noise to an image and try to bring it back without any conditioning. The goal is to get back an image very close to the test image. Once again, we try this with a variety of noise levels.
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1_7.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Test Image</figcaption>
      </td>
  </table>
</div>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1_7_testdoe.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Doe Library</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1_7.testblackwell.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Blackwell Hall</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.7.1: Editing Hand-Drawn and Web Images</h3>
Similar to the last part, we use the same prompt embeddings and iteratively denoise with cfg. However, now I’ve downloaded an image of Mike Wazowski and ran the forward method on it. We then run the method with varying i_start noise levels, ultimately showing that progression of images that leads to a Wazowski by the time it is 20. At the beginning with lower noise levels, there’s drastically different images.
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.7.1_mike.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Mike from Monsters, Inc.</figcaption>
      </td>
  </table>
</div>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.7.1_handhouse.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Hand-Drawn House</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1.7.1_handface.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Hand-Drawn Face</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.7.2: Inpainting</h3>
In this section, we have an image and a binary mask. The goal is to keep the image intact where the mask doesn’t exist and input a new image into the mask. In the example below, we have the Campanile whose top is masked and a new image has to be processed. We use a lot of logic from the iteratively denoised cfg method with some modifications at the end. We take the result of each iteration and then run the formula below with the original image. This becomes the new image we work on every iteration of the loop.
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.7.2_campanile.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Campanile</figcaption>
      </td>
  </table>
</div>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/test1.7.2_mikemask.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Mike image + mask</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/test1.7.2_mikeres.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Inpainted result for Mike</figcaption>
      </td>
  </table>
<br>
  <div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/test1.7.2_bldgmask.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Blackwell Hall img + mask</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/test1.7.2_bldgres.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Inpainted result for Blackwell Hall</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.7.3: Text-Conditional Image-to-image Translation</h3>
The logic of this section is essentially the same as 1.7.2, but instead we guide the projection with a text prompt by changing the prompt embedding. As you can see below, the image becomes more like what it should as the noise level increases.
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="proj5_images/part1.7.3_rocket.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Campanile</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.7.3_blackwell.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Blackwell Hall</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1.7.3_salesforce.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Salesforce Tower</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.8: Visual Anagrams</h3>
This was the coolest section to implement because we’re creating an optical illusion. At one orientation, the image looks like something, and then when it is flipped, it’s something entirely different. Once again, we use the logic behind iteratively denoising with cfg. Since we are creating two images in one, we need two conditional and unconditional prompt embeddings each. We use a pair containing one each to run the Unet twice and calculate that specific noise estimate, and then do it again on the second pair. When calculating these, we use the below formula. Most importantly, we must flip the image for the second Unet and then flip the outputs back. Once we have our two noise estimates, we calculate cfg noise estimate of each and then average those. That will be the noisy estimate for the image calculation.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/algorithm1.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Part 1.8's Algorithm</figcaption>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.8_oldman1.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"an oil painting of an old man"</figcaption>
      </td>
	<td>
        <img src="proj5_images/part1.8_campfire1.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"an oil painting of people around a campfire"</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.9_skull4.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a lithograph of a skull"</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1.9_snowy4.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"an oil painting of a snowy mountain village"</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.8_dog2.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a photo of a dog"</figcaption>
      </td>
 	</td>
	<td>
        <img src="proj5_images/part1.8_hipster2.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a photo of a hipster barista"</figcaption>
      </td>
  </table>
</div>

<h3 align="middle">Part 1.9: Hybrid Anagrams</h3>
In this section, our goal is to trick the viewer into seeing a certain image up close and a different image further away. Once again, we use the cfg noise estimate from two pairs of conditional and unconditional prompt embeddings. To pull off this trick, for the image we want to see up close, we use a low pass filter and then for the one we want to see further away, we use a high pass filter. We combine these filters to get a new noise estimate and then continue our code. The formula is also below for reference.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/algorithm2.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Part 1.9's Algorithm</figcaption>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.10_skullwaterfall.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a lithograph of a skull" and "a lithograph of waterfalls"</figcaption>
      </td>
	<td>
        <img src="proj5_images/part1.10_rocketpencil.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a rocket ship" and "a pencil"</figcaption>
      </td>
	<td>
        <img src="proj5_images/part1.10_skulldog.PNG" align="middle" width="400px"/>
        <figcaption align="middle">"a lithograph of a skull" and "a photo of a dog"</figcaption>
      </td>
  </table>
</div>

<h1 align="middle">Part B: Diffusion Models from Scratch!</h1>

<h3 align="middle">Part 1: Training a Single-Step Denoising UNet</h3>
In this part, our goal is to train a denoiser such that we can take in a noisy image and obtain a clean image by optimizing the L2 Loss. The Unet architecture has a lot of operations that we needed to implement such as ConvBlock and UpBlock. These operations had to be put in a specific order with the appropriate number of input and output channels. Once that was completed, we ran 5 epochs to train the model and all the results are below.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2_allnoise.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	</td>
	<td>
        <img src="proj5_images/part1.2.1_graph.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Training Loss Curve Plot</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2.1_epoch0.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Results after epoch 0</figcaption>
	</td>
	<td>
        <img src="proj5_images/part1.2.1_epoch5.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Results after epoch 5</figcaption>
      </td>
  </table>
</div>
<br>
Below is the test digit on varying levels of noises. The last one is for when it is at 1.0, the screenshot was cut off.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2.2_0.0noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	</td>
	<td>
        <img src="proj5_images/part1.2.2_0.2noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2.2_0.4noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	</td>
	<td>
        <img src="proj5_images/part1.2.2_0.5noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2.2_0.6noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	</td>
	<td>
        <img src="proj5_images/part1.2.2_0.8noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part1.2.2_1.0noise4.PNG" align="middle" width="400px"/>
        <figcaption align="middle"></figcaption>
	</td>
  </table>
</div>

<h3 align="middle">Part 2: Training a Diffusion Model</h3>

First, we had time conditioning to the Unet, which involves adding two FCBlocks based on t. We then add these results to the appropriate block based on the image from the spec. At this point, when we sample, we only include the unconditional value for epsilon. In part 2.4, we add class-conditioning to the Unet, which requires adding two more FCBlocks that use the c paramter. Essentially we want a 10% dropout rate, where c gets set to zeros. We multiply the result of the FCBlock before adding the time conditioning result from the last part.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part2.2_graph.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Training Loss Curve Plot</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part2.3_deliv.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Results after epoch 0 (top) & epoch 5 (bottom)</figcaption>
	</td>
  </table>
</div>
After implementing class conditioning results below.
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part2.4_graph.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Training Loss Curve Plot</figcaption>
      </td>
  </table>
</div>
<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="proj5_images/part2.5_epoch520.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Results after epoch 0 (top 4 rows) & epoch 5 (bottom 4 rows)</figcaption>
	</td>
  </table>
</div>

<body>

</body>
</html>
